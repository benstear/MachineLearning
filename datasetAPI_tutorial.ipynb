{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "from tensorflow.data import Dataset as Ds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data  =  pd.read_csv('/Users/dawnstear/desktop/chop_cellpred/data.csv')\n",
    "data_shuffled = shuffle(data)\n",
    "X_ = data_shuffled.drop(['Labels','TYPE'],axis=1)\n",
    "y_ = data_shuffled['Labels'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_.values,y_.values,test_size=0.2,random_state=144)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                   \n",
    "BUFFER = 47\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Create dataset iterator to avoid using feed_dict() (its very slow)\n",
    "# TRAIN DATASET\n",
    "X_train_dataset = Ds.from_tensor_slices((X_train))   \n",
    "y_train_dataset = Ds.from_tensor_slices((y_train)).map(lambda z: tf.one_hot(z, 10))\n",
    "train_dataset = Ds.zip((X_train_dataset, y_train_dataset)).shuffle(BUFFER).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "# Create dataset iterator to avoid using feed_dict() (its very slow)\n",
    "# TEST DATASET\n",
    "X_test_dataset = Ds.from_tensor_slices((X_test)) \n",
    "y_test_dataset = Ds.from_tensor_slices((y_test)).map(lambda z: tf.one_hot(z, 10))\n",
    "test_dataset = Ds.zip((X_test_dataset, y_test_dataset)).shuffle(BUFFER).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# create general iterator, seamlessly switch bt train data and test data sets\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "\n",
    "# This will return a tuple where next_element[0] = data, next_element[1] = labels\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "testing_init_op = iterator.make_initializer(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(in_data):\n",
    "    bn = tf.layers.batch_normalization(in_data)\n",
    "    fc1 = tf.layers.dense(bn, 50)\n",
    "    fc2 = tf.layers.dense(fc1, 50)\n",
    "    fc2 = tf.layers.dropout(fc2)\n",
    "    fc3 = tf.layers.dense(fc2, 10)\n",
    "    return fc3\n",
    "\n",
    "# create the neural network model\n",
    "logits = nn_model(next_element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits, 1)\n",
    "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 336.268, training accuracy: 3.33%\n",
      "Epoch: 50, loss: 15.324, training accuracy: 90.00%\n",
      "Epoch: 100, loss: 0.893, training accuracy: 96.67%\n",
      "Epoch: 150, loss: 0.001, training accuracy: 100.00%\n",
      "Epoch: 200, loss: 0.003, training accuracy: 100.00%\n",
      "Epoch: 250, loss: 0.001, training accuracy: 100.00%\n",
      "Epoch: 300, loss: 0.000, training accuracy: 100.00%\n",
      "Epoch: 350, loss: 0.000, training accuracy: 100.00%\n",
      "Epoch: 400, loss: 0.000, training accuracy: 100.00%\n",
      "Epoch: 450, loss: 0.000, training accuracy: 100.00%\n",
      "Epoch: 500, loss: 0.000, training accuracy: 100.00%\n",
      "Epoch: 550, loss: 0.000, training accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'validation_init_op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-61badc8bb23e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# re-initialize the iterator, but this time with validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_init_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_init_op' is not defined"
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "epochs = 600\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    for i in range(epochs):\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        if i % 50 == 0:\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "    # now setup the validation run\n",
    "    valid_iters = 100\n",
    "    # re-initialize the iterator, but this time with validation data\n",
    "    sess.run(testing_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(valid_iters):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters,\n",
    "                                                                                 (avg_acc / valid_iters) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
